{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Hyperparameter Tuning\n",
    "\n",
    "\n",
    "\n",
    "### Key Take-Aways\n",
    "- Generalization _(How well the model applies to observations not seen during learning)_\n",
    "- Overfitting\n",
    "- Underfitting\n",
    "- How to measure classification methods, _score as a normal variable_\n",
    "    - Resubstitution - never\n",
    "    - Hold-Out - for large datasets\n",
    "    - k-fold Cross validation - best option\n",
    "- Hyperparamter Tuning and Leakage\n",
    "    - How to automatically find good hyperparameters\n",
    "    - Beware of Data Leakage\n",
    "- Nested Hold-Out & Nested Cross Validation\n",
    "    - For Hyperparameter Tuning without Data Leakage\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Resubstitution\n",
    "Use the whole data set for training and testing. Very bad since it just leads to overfitting. Usage: Never\n",
    "\n",
    "#### Hold-out\n",
    "Split data into a train and test set. Only works if there is enough labelled data and no way to measure variance. Common split 80/20. Train data only with training set. Perform metrics on both sets. Compare those metrics between the sets. Model is,\n",
    "- **Underfitting**, if both are bad\n",
    "- **Overfitting**, if good on train and bad on test\n",
    "- Well **generalizing**, if good on both  \n",
    "\n",
    "#### k-fold cross-validation\n",
    "Split dataset into $k$ smaller _dataset-folds_, build model out of each, build score for each model. Take average of all scores. This method is better suited for small datasets.\n",
    "\n",
    "#### Fitting Graph to avoid Overfitting\n",
    "Used to find the _optional_ model complexity visually. The Graph here shows the error on the y-axis, it's also possible to show the accuracy as alternative. The Ideal point is the turning point on the testing data graph.\n",
    "\n",
    "![Fitting Graph](img/Fitting_Graph.png)\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "The challenge with hyperparameter tuning is, evaluating the quality of the hyperparameters without causing a **Data Leakage**. A Data Leakage would mean the same data is used for testing and training. Solution, split data further into `train dataset`, `validation dataset` and `test dataset`. \n",
    "\n",
    "**Advantage**\n",
    "- Test dataset is never used to train/tune â†’ no data leakage\n",
    "- Good Performance\n",
    "- Easy to implement\n",
    "\n",
    "**Disadvantages**\n",
    "- Need lots of labelled data, not suited for small datasets\n",
    "- No way to measure variance\n",
    "- Often pessimistic estimation of true score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "43a903fe36b38cc5bc3c6ffc5b75ae50bab50c8d56bc914f66ebc140ba6708ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
